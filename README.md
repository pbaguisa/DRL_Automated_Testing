# ğŸ¤– Deep Reinforcement Learning for Automated Testing

## ğŸ§­ Overview
This project explores the automation of **application testing** using **Deep Reinforcement Learning (DRL).** AI agents were trained using two algorithmsâ€”**PPO (Proximal Policy Optimization)** and **A2C (Advantage Actor-Critic)**â€”within a custom environment called the **Bubble Game.**

The goal is to evaluate how well different DRL agents can **learn to test an application autonomously**, simulating two different behaviour modes:

ğŸ›¡ï¸ Survivor Mode â€“ Prioritizes longevity, avoiding risky behaviors.
âš¡ Speedrunner Mode â€“ Prioritizes aggressive, high-throughput interaction and speed.

## ğŸ¯ Project Objectives
- Develop a custom **Game Environment** for reinforcement learning experiments
- Train and compare **PPO** and **A2C** agents using **Stable Baselines3**
- Explore how different **reward structures** (Survivor v. Speedrunner) affect agent behaviour and performance
- Log and visualize results using **Tensorboard** and **Matplotlib**

## ğŸ§® Algorithms Used

**PPO (Proximal Policy Optimization)**
- Stable and sample-efficient on-policy algorithm
- Performs clipped updates to prevent large, unstable policy changes

**A2C (Advantage Actor-Critic)**
- Simpler actor-critic method that updates after each rollout
- Faster but typically less stable than PPO
Both algorithms were implemented using the **Stable Baselines3 library**.

## ğŸ® Game Modes

ğŸ›¡ï¸ Survivor Mode
- Rewards for longevity, survival, and avoiding collisions
- Penalties for idling or being too passive
- Encourages conservative movement and survival strategy

âš¡ Speedrunner Mode
- Rewards for rapid shooting and high throughput
- Penalties for inactivity or taking too long to complete tasks
- Encourages fast, aggressive gameplay and efficiency

## âš™ï¸ Setup

### ğŸ§© Create/Start Virtual Environment
    python -m venv venv
    .\venv\Scripts\activate

### ğŸ“¦ Install libraries
    pip install gym[all] stable_baselines3 typing numpy tensorboard numpy pandas matplotlib

### ğŸ§  To run training:
    python src/train.py --algo ppo --reward_mode survivor --timesteps 200000 --seed 7

### ğŸ§ª To evaluate trained models:
    python src/eval.py --model_path models/ppo_bubble_survivor_seed7 --reward_mode survivor --episodes 20 --csv_out logs/ppo_survivor.csv

### ğŸ¥ To visualize: 
    python src/visualize.py --model_path models/ppo_bubble_survivor_seed7 --reward_mode survivor --fps 60

### ğŸ“Š To view Tensorboard:
    tensorboard --logdir logs

## ğŸŒ Environment 
### ğŸ® Actions
| ID | Meaning                   |
| -: | ------------------------- |
|  0 | Move Left                 |
|  1 | Move Right                |
|  2 | Shoot (cooldown 2 frames) |
|  3 | No-op                     |

### ğŸ‘ï¸ Observations (shape = 8, float32)
| Indexes | Description                          |
| ------- | ------------------------------------ |
| 0â€“1     | Player `(x, y)`                      |
| 2â€“3     | Bullet `(x, y)` (zeros if no bullet) |
| 4â€“5     | Bubble 1 `(x, y)`                    |
| 6â€“7     | Bubble 2 `(x, y)`                    |

### ğŸ’° Rewards (by Mode)
| Event / Term                                 |                                   Survivor Mode |                                Speedrunner Mode |
| -------------------------------------------- | ----------------------------------------------: | ----------------------------------------------: |
| **Shoot**                                    |                                         `+0.50` |                                         `+0.75` |
| **Per-step time shaping**                    |                           `+0.05` (alive bonus) |                          `-0.01` (step penalty) |
| **Align gain**                               |             `+0.002 Ã— (WIDTH âˆ’ min(dx, WIDTH))` |             `+0.004 Ã— (WIDTH âˆ’ min(dx, WIDTH))` |
| **Proximity bonus** *(under bubble & close)* |                          `+0.10` *(SAFE_BONUS)* |                         `+0.15` *(CLOSE_BONUS)* |
| **Wall penalty**                             |            `-0.05` per step *(left/right wall)* |                                               â€” |
| **Bullet drag**                              |       `-0.005` per step *(while bullet exists)* |                                               â€” |
| **Pop bubble**                               |                                         `+10.0` |                                         `+20.0` |
| **Death (player collision)**                 |                                        `-100.0` |                                        `-100.0` |
| **Truncation**                               | `max_steps=200000` â†’ no special terminal reward | `max_steps=200000` â†’ no special terminal reward |

Here's the aligned version of your table:

## âš–ï¸ Algorithm Configuration
| Hyperparameter    |        PPO (used) |        A2C (used) | Notes                        |
|-------------------|------------------:|------------------:|------------------------------|
| `policy`          |       `MlpPolicy` |       `MlpPolicy` | Feed-forward MLP             |
| `n_steps`         |              2048 |                â€”  | Rollout length before update |
| `batch_size`      |               256 |                â€”  | Larger â†’ stabler gradients   |
| `gamma`           |     0.995 â†’ 0.999 |              0.99 | Long-term reward weighting   |
| `gae_lambda`      |       0.98 â†’ 0.99 |              1.00 | Advantage estimation         |
| `learning_rate`   |       3e-4 â†’ 1e-4 |       7e-4 â†’ 6e-4 | Lower = more stable          |
| `total_timesteps` |           200,000 |           200,000 | Training budget              |
| `seed`            |                 7 |                 7 | Reproducibility              |

## ğŸ¬ PPO Agent in Speedrunner Mode shooting bubbles
<img src="https://github.com/user-attachments/assets/7c45bec4-f6a4-454a-a4f6-fe6495cc0e19" width="650" alt="game_example">

