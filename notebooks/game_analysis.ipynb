{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbac5d4c",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Automated Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84df5c53",
   "metadata": {},
   "source": [
    "To explore automated application testing with deep reinforcement learning (DRL), two types of agents were trained using **PPO (Proximal Policy Optimization)** and **A2C (Advantage Actor-Critic)** algorithms in a custom \"Bubble Game\" environment. The reward function provided incentives for desired behaviours, such as: shooting (regardless of hitting a target), aligning with bubbles,\n",
    "sucessfully popping bubbles. While penalizing undesirable actions such as colliding with bubbles, idling, wall-camping, and bubble drag. \n",
    "\n",
    "Two reward configurations were designed to operationalize distinct testing strategies. **Survivor mode** prioritizes conservative navigation and sustained episode length by imposing heavier penalties on risky actions, whereas **Speedrunner** mode incentivizes aggressive, high-throughput interaction via larger rewards for rapid bubble pops and frequent movement.\n",
    "\n",
    "To ensure fair and reproducible comparisons, four models were trained in a virtual environment under an identical random seed, with matched environment settings and hyperparameters. Following training, a dedicated evaluation script recorded performance metrics, which were subsequently parsed into a CSV file for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe2f98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Imports + Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4335071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    episode       reward  shots  pops  deaths  frames_alive  wall_ratio  \\\n",
      "0         1  2072.563546      8     1       1          1703    0.560188   \n",
      "1         2    32.009345      0     0       1           109    0.000000   \n",
      "2         3    61.959087      0     0       1           116    0.931034   \n",
      "3         4  1039.152390      1     0       1          1008    0.690476   \n",
      "4         5   699.644869      2     1       1           552    0.588768   \n",
      "5         6   300.398562      0     0       1           354    0.471751   \n",
      "6         7  2383.205031      4     1       0          2000    0.606500   \n",
      "7         8   948.324027      1     1       1           726    0.648760   \n",
      "8         9   877.317851      0     0       1           833    0.786315   \n",
      "9        10  2000.709654      0     0       1          1693    0.943296   \n",
      "10       11   -30.283749      0     0       1            50    0.000000   \n",
      "11       12    -7.983874      0     0       1            60    0.000000   \n",
      "12       13  1269.225799      3     1       1          1040    0.587500   \n",
      "13       14   674.043620      4     1       1           559    0.243292   \n",
      "14       15    54.275170      0     0       1           107    0.000000   \n",
      "15       16    -3.774832      1     0       1            69    0.000000   \n",
      "16       17    25.862080      2     1       1            80    0.000000   \n",
      "17       18  1926.198218      0     0       0          2000    0.587000   \n",
      "18       19   -13.501680      0     0       1            58    0.517241   \n",
      "19       20   -19.613330      0     0       1            56    0.000000   \n",
      "\n",
      "    accuracy    avg_dist reward_mode  \n",
      "0   0.125000  176.195377    survivor  \n",
      "1   0.000000  211.883738    survivor  \n",
      "2   0.000000   96.728074    survivor  \n",
      "3   0.000000  242.106949    survivor  \n",
      "4   0.500000   94.135082    survivor  \n",
      "5   0.000000  245.411635    survivor  \n",
      "6   0.250000  216.824992    survivor  \n",
      "7   1.000000   92.872571    survivor  \n",
      "8   0.000000  217.756392    survivor  \n",
      "9   0.000000  180.534656    survivor  \n",
      "10  0.000000  111.337493    survivor  \n",
      "11  0.000000   44.448949    survivor  \n",
      "12  0.333333  156.470770    survivor  \n",
      "13  0.250000  135.104096    survivor  \n",
      "14  0.000000   96.377709    survivor  \n",
      "15  0.000000  117.426321    survivor  \n",
      "16  0.500000   94.362002    survivor  \n",
      "17  0.000000  328.775446    survivor  \n",
      "18  0.000000   52.600690    survivor  \n",
      "19  0.000000   92.529729    survivor  \n"
     ]
    }
   ],
   "source": [
    "ppo_survivor_data = pd.read_csv('logs/ppo_survivor.csv')\n",
    "ppo_speedrunner_data = pd.read_csv('logs/ppo_speedrunner.csv')\n",
    "a2c_survivor_data = pd.read_csv('logs/a2c_survivor.csv')\n",
    "a2c_speedrunner_data = pd.read_csv('logs/a2c_speedrunner.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31df56",
   "metadata": {},
   "source": [
    "### Bubble Game: PPO vs A2C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
